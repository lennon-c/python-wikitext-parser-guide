{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"A Hands-On Guide to Parsing Wikitext","text":"<p>In this a hands-on guide on how to parse wikitext from beginning to end using Python. </p>"},{"location":"#what","title":"What?","text":"<p>Throughout the tutorial, we will use the German Wiktionary as our example. </p> <p>This hands-on guide is divided into three parts:</p> <ol> <li> <p>Fetching the Data: Learn two ways to retrieve wiki data \u2014 by using the Special Export tool or by downloading wiki dump files. Learn more about fetching XML data.</p> </li> <li> <p>Parsing the XML Files: Once the data is retrieved in XML format, this section explains how to parse the files to extract the wikitext. Learn more about parsing XML files.</p> </li> <li> <p>Parsing the Wikitext: In the final part, we will parse the wikitext and extract elements such as headings, sections, word forms, meanings, inflections and more. Learn more about parsing wikitext.</p> </li> </ol>"},{"location":"#requirements-to-follow-along","title":"Requirements to follow along","text":"<p>To follow along with this tutorial, you will need to have the following Python packages installed:</p> <ul> <li><code>requests</code></li> <li><code>lxml</code></li> <li><code>mwparserfromhell</code></li> </ul> <p>You can install them by running the following command:</p> Bash<pre><code>pip install requests lxml mwparserfromhell\n</code></pre> <p>You can now start with the hands-on tutorial right away or continue reading below (optional)! </p>"},{"location":"#why","title":"Why?","text":"<p>Why would you want to extract content using wikitext instead of scraping wiki pages?</p> <ul> <li> <p>Webpage layouts change frequently. If you are scraping the web page, any layout change can break your code. This actually happened to me (see below). However, if your code relies on wikitext content, it is less likely to be affected by these changes.</p> <ul> <li>Wikitext is the source content and contains only minimal formatting. This content then undergoes several transformations for final formatting. One transformation occurs when templates are applied (referred to as \u201ctransclusion\u201d in wiki jargon), and another when CSS styles are added.</li> </ul> </li> <li> <p>Access to Dump files. Since dump files store content in wikitext format, parsing wikitext is the only option here. Working with dump files has the following advantages:  </p> <ul> <li>Dump files are typically downloaded once, so you avoid repeated access to Wikipedia servers. This is respectful of Wikimedia\u2019s resources and helps avoid IP blocking or rate-limit issues.  </li> <li>With the entire database on your computer, you have a broad range of data analysis options available.  </li> <li>No matter how many changes occur on the live site, your code will always work with the dump file.</li> </ul> </li> </ul> <p>That being said, parsing wikitext does come with its own challenges:</p> <ul> <li> <p>Understanding wikitext basics: To work with wikitext, you will need to learn some of its syntax and formatting conventions.</p> </li> <li> <p>Differences from the live web page: The wikitext content you parse is not always identical to what you see on the live web page.</p> </li> </ul>"},{"location":"#the-story-behind-this-tutorial","title":"The Story Behind This Tutorial","text":"<p>Last year, I set up a system of flashcards for learning German, which involved scraping German Wiktionary pages to retrieve word inflections using BeautifulSoup. My code worked well initially, but by the time I wanted to use it again, it was broken because the website layout had changed. So, I fixed the code, but when the layout changed again, I realized that web scraping was not the best approach for the task at hand.</p> <p>I knew that Wiki projects maintain dump files, so I thought I would give those a try. This was easier said than done.</p> <p>The dump files do not store content in HTML format, but rather in a markup language called wikitext, which is what Wiki contributors use to add content. Additionally, while web scraping requires little knowledge about the structure of Wiki projects, working with dump files does require some understanding of their format. For instance, you need to know what a Wiki namespace is, understand versioning, have a grasp of wikitext syntax, and be able to deal with templates. Moreover, dump files are large, and trial-and-error approaches can be time-consuming.</p> <p>I would have certainly benefited from a tutorial that explains these essential aspects of parsing wikitext without needing to become a wikitext parsing expert myself. Writing this tutorial is my way of giving back to the Python and open-source community for the countless hours of joy and learning.</p> <p>And also because open source is the Robin Hood of our time, in my humble opinion :)</p> <p>Happy coding! c-lennon</p>"},{"location":"pycon_at/","title":"Workshop","text":""},{"location":"pycon_at/#a-hands-on-workshop-on-parsing-wikitext","title":"A Hands-On Workshop on Parsing Wikitext","text":"<p>PyCon Austria 6 and 7 April 2025 | Eisenstadt</p> <p>In this workshop, you will learn how to fetch, parse, and extract data from the German Wiktionary to gather linguistic information such as parts of speech, inflections, example sentences, and definitions. You will also learn where to find the data and how to process it using HTTP requests, as well as XML and Wikitext parsers.</p> <p>Fetching XML data</p> <ul> <li>Special Exports - go to Google Colab</li> <li>Dump files - go to guide</li> </ul> <p>Parsing XML</p> <ul> <li>Parsing XML from Special Export - go to Google Colab</li> <li>Parsing XML from Dump file - go to Google Colab</li> </ul> <p>Parsing Wikitext</p> <ul> <li>Wiki Basics - go to guide</li> <li>Parsing Wikitext - go to Google Colab</li> </ul>"},{"location":"pycon_at/#using-google-colab","title":"Using Google Colab","text":"<p>In this workshop, we will use Google Colab, a free, cloud-based platform for running Python code in a Jupyter Notebook environment. </p> <p>Follow these steps to get started:</p> <ol> <li>Make sure you are signed in with your Google account.</li> <li>Ensure that you have an active Internet connection on your device.</li> <li>You can follow and run the code on a tablet, but if you'd like to edit on the go, I recommend using a laptop for a smoother experience.</li> <li>To access the workshop material, simply click the links provided in the guide table of contents.</li> </ol>"},{"location":"pycon_at/#running-on-your-own-machine","title":"Running on Your Own Machine","text":"<p>If you prefer not to use Google Colab but still want to participate in the workshop, you can download the source code and run it on your local machine.</p> <p>To get started, make sure you install the necessary dependencies for the workshop:</p> <ul> <li><code>requests</code>, <code>lxml</code> and <code>mwparserfromhell</code></li> </ul> Text Only<pre><code>pip install requests lxml mwparserfromhell\n</code></pre>"},{"location":"pycon_at/#download-materials-of-the-workshop","title":"Download Materials of the Workshop","text":"<p>You can download the zip file with all the materials of the workshop, including:</p> <ul> <li>Jupyter Notebooks</li> <li>Python files</li> <li>Sample data</li> </ul> <p>Download zip file</p>"},{"location":"pycon_template/","title":"Pycon template","text":""},{"location":"pycon_template/#a-hands-on-workshop-on-parsing-wikitext","title":"A Hands-On Workshop on Parsing Wikitext","text":"<p>PyCon Austria 6 and 7 April 2025 | Eisenstadt</p> <p>In this workshop, you will learn how to fetch, parse, and extract data from the German Wiktionary to gather linguistic information such as parts of speech, inflections, example sentences, and definitions. You will also learn where to find the data and how to process it using HTTP requests, as well as XML and Wikitext parsers.</p> <p>[INSERT]</p>"},{"location":"pycon_template/#using-google-colab","title":"Using Google Colab","text":"<p>In this workshop, we will use Google Colab, a free, cloud-based platform for running Python code in a Jupyter Notebook environment. </p> <p>Follow these steps to get started:</p> <ol> <li>Make sure you are signed in with your Google account.</li> <li>Ensure that you have an active Internet connection on your device.</li> <li>You can follow and run the code on a tablet, but if you'd like to edit on the go, I recommend using a laptop for a smoother experience.</li> <li>To access the workshop material, simply click the links provided in the guide table of contents.</li> </ol>"},{"location":"pycon_template/#running-on-your-own-machine","title":"Running on Your Own Machine","text":"<p>If you prefer not to use Google Colab but still want to participate in the workshop, you can download the source code and run it on your local machine.</p> <p>To get started, make sure you install the necessary dependencies for the workshop:</p> <ul> <li><code>requests</code>, <code>lxml</code> and <code>mwparserfromhell</code></li> </ul> Text Only<pre><code>pip install requests lxml mwparserfromhell\n</code></pre>"},{"location":"pycon_template/#download-materials-of-the-workshop","title":"Download Materials of the Workshop","text":"<p>You can download the zip file with all the materials of the workshop, including:</p> <ul> <li>Jupyter Notebooks</li> <li>Python files</li> <li>Sample data</li> </ul> <p>Download zip file</p>"},{"location":"Fetching%20XML%20data/","title":"Fetching XML","text":"<p>In the first section, we will cover two ways of accessing the XML files that contain Wikitext.</p> <ul> <li>First, we will access them online using the Wiki Special Export tool.</li> <li>Next, we will learn where to find the Wiki Dump File.</li> </ul> <p>Notice that you can use the <code>Previous</code> and <code>Next</code> links in the footer to navigate forward or backward throughout the hands-on tutorial.</p> <p>Let us begin by exploring the Special Export tool method.</p>"},{"location":"Fetching%20XML%20data/Dump%20files/","title":"Dump File","text":"<p>Dump File Fetching: This is a static snapshot of all wiki pages, which is stored in a single compressed file (e.g., dewiktionary-latest-pages-articles.xml.bz2). </p> On this page<ul> <li>German Wiktionary Dump Files<ul> <li>Latest Version</li> <li>Latest (alternative way) and Older Versions</li> </ul> </li> <li>Any Wiki Dump File</li> <li>Should I download a multistream dump file or not?</li> </ul>"},{"location":"Fetching%20XML%20data/Dump%20files/#german-wiktionary-dump-files","title":"German Wiktionary Dump Files","text":""},{"location":"Fetching%20XML%20data/Dump%20files/#latest-version","title":"Latest Version","text":"<ul> <li>To download the latest dump file of the German Wiktionary, click here.<ul> <li>This will download the compressed file: <code>dewiktionary-latest-pages-articles-multistream.xml.bz2</code>.</li> <li>The file is stored in this directory: https://dumps.wikimedia.org/dewiktionary/latest/.</li> </ul> </li> </ul>"},{"location":"Fetching%20XML%20data/Dump%20files/#latest-alternative-way-and-older-versions","title":"Latest (alternative way) and Older Versions","text":"<ul> <li>If you need an older version of the Wiktionary dump, visit this directory: https://dumps.wikimedia.org/dewiktionary/.<ul> <li>To download a specific version:<ul> <li>Navigate to the folder for the desired date.</li> <li>A new window will open.</li> <li>Look for the section titled Articles, templates, media/file descriptions, and primary meta-pages.</li> <li>Select the file. The file name will follow the pattern: <code>dewiktionary-YYYYMMDD-pages-articles.xml.bz2</code>, where <code>YYYYMMDD</code> represents the dump date.</li> </ul> </li> <li>Notes:<ul> <li>Older dumps are only retained for a few months.</li> <li>You can also fetch the latest version from this directory by choosing the most recent date.</li> </ul> </li> </ul> </li> </ul>"},{"location":"Fetching%20XML%20data/Dump%20files/#any-wiki-dump-file","title":"Any Wiki Dump File","text":"<ul> <li>Click on Database backup dumps.</li> <li>Scroll down the page to find the domain of interest.<ul> <li>For example, use <code>enwiktionary</code> for the English Wiktionary.</li> </ul> </li> <li>Click on the domain link, then look for the section titled Articles, templates, media/file descriptions, and primary meta-pages.<ul> <li>The file you are looking for should end with <code>-pages-articles.xml.bz2</code>.</li> </ul> </li> </ul>"},{"location":"Fetching%20XML%20data/Dump%20files/#should-i-download-a-multistream-dump-file-or-not","title":"Should I download a multistream dump file or not?","text":"<p>The files <code>-pages-articles.xml.bz2</code> and <code>multistream-pages-articles.xml.bz2</code> contain the same information. For our purposes here, either option will work just fine because we are working with a relatively small wiki database.</p> <p>However, if you plan to work with a larger dump file in the future that exceeds your computer's memory capacity, you could download the <code>multistream-pages-articles.xml.bz2</code> version. This would allow you to adjust your parsing strategy to process the data in smaller chunks.</p>"},{"location":"Fetching%20XML%20data/Special%20Exports/","title":"Special Export tool","text":"<p>The Special Export tool fetches specific pages with their raw content (wikitext) in real-time, without needing to download the entire dataset. The content is provided in XML format.</p> On this page<ul> <li>Importing Packages</li> <li>Using the Special Export Tool</li> <li>Fetching XML Data with requests</li> </ul>"},{"location":"Fetching%20XML%20data/Special%20Exports/#importing-packages","title":"Importing Packages","text":"Python<pre><code>import requests # to fetch info from URLs\n</code></pre>"},{"location":"Fetching%20XML%20data/Special%20Exports/#using-the-special-export-tool","title":"Using the Special Export Tool","text":"<p>You can actually use Special:Export to retrieve pages from any Wiki site. On the German Wiktionary, however, the tool is labelled Spezial:Exportieren, but it works the same way.</p> <p>Exporting Pages from Any Wiki Site</p> <p>To access the XML content of the page titled \"Austria\" from English Wikipedia, you can construct your URL as follows. </p> SourceResult Python<pre><code>title = 'Austria'\ndomain = 'en.wikipedia.org'\nurl = f'https://{domain}/wiki/Special:Export/{title}'\nprint(url)\n</code></pre> Python Console Session<pre><code>https://en.wikipedia.org/wiki/Special:Export/Austria\n</code></pre> <p>Exporting Pages from the German Wiktionary</p> <p>For the German Wiktionary, the export tool uses <code>Spezial:Exportieren</code> instead of <code>Special:Export</code>. </p> SourceResult Python<pre><code>title = 'hoch'\ndomain = 'de.wiktionary.org'\nurl = f'https://{domain}/wiki/Spezial:Exportieren/{title}'\nprint(url)\n</code></pre> Python Console Session<pre><code>https://de.wiktionary.org/wiki/Spezial:Exportieren/hoch\n</code></pre>"},{"location":"Fetching%20XML%20data/Special%20Exports/#fetching-xml-data-with-requests","title":"Fetching XML Data with <code>requests</code>","text":"<p>To programmatically fetch and download XML content, you can use Python's <code>requests</code> library. This example shows how to build the URL, make a request, and get the XML content of a Wiktionary page by its title.</p> Python<pre><code>def fetch(title):\n    # Construct the URL for the XML export of the given page title\n    url = f'https://de.wiktionary.org/wiki/Spezial:Exportieren/{title}'\n\n    # Set User-Agent header\n    headers = {\n        \"User-Agent\": \"Search for German words (https://lennon-c.github.io/python-wikitext-parser-guide)\"\n    }\n\n    # Send a GET request\n    resp = requests.get(url, headers=headers)\n\n    # Check if the request was successful, and raise an error if not\n    resp.raise_for_status()\n\n    # Return the XML content of the requested page\n    return resp.text\n</code></pre> <p>Next, let us attempt to retrieve the XML content for the page titled \"hoch\" and print the initial 500 bytes for a glimpse of the XML content.</p> SourceResult Python<pre><code>page = fetch('hoch')\nprint(page[:500])\n</code></pre> Python Console Session<pre><code>&lt;mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.11/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.11/ http://www.mediawiki.org/xml/export-0.11.xsd\" version=\"0.11\" xml:lang=\"de\"&gt;\n  &lt;siteinfo&gt;\n    &lt;sitename&gt;Wiktionary&lt;/sitename&gt;\n    &lt;dbname&gt;dewiktionary&lt;/dbname&gt;\n    &lt;base&gt;https://de.wiktionary.org/wiki/Wiktionary:Hauptseite&lt;/base&gt;\n    &lt;generator&gt;MediaWiki 1.46.0-wmf.7&lt;/generator&gt;\n    &lt;case&gt;case-sensitive&lt;/case&gt;\n    &lt;namespa\n</code></pre> <p>We will continue to use the <code>fetch</code> function throughout this tutorial.</p>"},{"location":"Parsing%20Wikitext/","title":"Parsing Wikitext","text":"<p>We are now at the final section of this hands-on tutorial, where we will parse the Wikitext itself.</p> <ul> <li>First, we will review key Wiki concepts and Wikitext syntax in Wiki Basics.</li> <li>Then, we will use <code>mwparserfromhell</code> and regular expressions to extract elements such as headings, sections, parts of speech, declensions, meanings, and more in Parsing Wikitext.</li> </ul> <p>Let us begin by reviewing the Wiki Basics.</p>"},{"location":"Parsing%20Wikitext/Parsing%20Wikitext/","title":"Parsing Wikitext","text":"On this page<ul> <li>Importing Packages</li> <li>Get the Wikitext Data</li> <li>Parsing Wikitext</li> <li>Retrieving Headings and Sectioning</li> <li>Extracting Templates<ul> <li>Wortart Template</li> <li>Conjugation or Declension Templates</li> </ul> </li> <li>Extracting Other Important Content</li> </ul>"},{"location":"Parsing%20Wikitext/Parsing%20Wikitext/#importing-packages","title":"Importing Packages","text":"Python<pre><code>import requests # to fetch info from URLs\nimport lxml.etree as ET # to parse XML data\nimport mwparserfromhell # to parse and analyze wikitext\nimport re # to extract information using regular expressions\nimport functools # to implement caching with a decorator\n</code></pre>"},{"location":"Parsing%20Wikitext/Parsing%20Wikitext/#get-the-wikitext-data","title":"Get the Wikitext Data","text":"<p>We already know how to extract wikitext from Dump files and the Special Exports tool. In this section, we will parse the wikitext.</p> <p>We will use the page titled <code>stark</code> (Wiktionary page) and the functions we created in the previous sections based on the Special Export method.</p> Python<pre><code>@functools.cache\ndef fetch(title):\n    url = f'https://de.wiktionary.org/wiki/Spezial:Exportieren/{title}'\n    headers = {\n        \"User-Agent\": \"Search for German words (https://lennon-c.github.io/python-wikitext-parser-guide)\"\n    }\n    resp = requests.get(url, headers=headers)\n    resp.raise_for_status()\n    return resp.text\n\ndef fetch_wikitext(title):\n    xml_content = fetch(title)\n    root = ET.fromstring(xml_content)\n    namespaces  = root.nsmap\n    page = root.find('page', namespaces)\n    wikitext = page.find('revision/text', namespaces)\n    return wikitext.text\n</code></pre> <p>I added <code>@functools.cache</code>, optional, to avoid redundant requests and be more respectful to the server. Basically, <code>@functools.cache</code> stores results of <code>fetch(title)</code>, so repeated calls with the same title return the cached response instead of requesting the page again from the wiki servers. </p> SourceResult Python<pre><code>wikitext = fetch_wikitext('stark')\nprint(wikitext[:1000])\n</code></pre> Python Console Session<pre><code>{{Siehe auch|[[st\u00e6rk]], [[st\u00e4rk]]}}\n== stark ({{Sprache|Deutsch}}) ==\n=== {{Wortart|Adjektiv|Deutsch}} ===\n\n{{Deutsch Adjektiv \u00dcbersicht\n|Positiv=stark\n|Komparativ=st\u00e4rker\n|Superlativ=st\u00e4rksten\n|Bild 1=Weight lifting black and white.jpg|mini|1|eine ''starke'' [[Frau]] beim [[Gewichtheben]]\n|Bild 2=Agonis Flexuosa - bark.jpg|mini|3|ein ''starker'' [[Baumstamm]]\n|Bild 3=Snow pile (3123493946).jpg|mini|4|Es hat ''stark'' [[schneien|geschneit]].\n}}\n\n{{Worttrennung}}\n:stark, {{Komp.}} st\u00e4r\u00b7ker, {{Sup.}} am st\u00e4rks\u00b7ten\n\n{{Aussprache}}\n:{{IPA}} {{Lautschrift|\u0283ta\u0281k}}\n:{{H\u00f6rbeispiele}} {{Audio|De-stark.ogg}}, {{Audio|De-stark2.ogg}}, {{Audio|De-at-stark.ogg|spr=at}}\n:{{Reime}} {{Reim|a\u0281k|Deutsch}}\n\n{{Bedeutungen}}\n:[1] mit [[Kraft]] [[ausgestattet]], von Kraft [[pr\u00e4gen|gepr\u00e4gt]], [[zeugen]]d\n:[2] [[hoch|hohe]] [[Leistung]] [[erbringen]]d; sehr [[leistungsf\u00e4hig]]\n:[3] [[\u00e4u\u00dfer-|\u00e4u\u00dferen]] [[Einfluss|Einfl\u00fcssen]], [[Belastung]]en [[standhalten]]d (aufgrund von [[Gr\u00f6\u00dfe]], [[Dicke]] oder \u00c4hnlichem)\n:[\n</code></pre>"},{"location":"Parsing%20Wikitext/Parsing%20Wikitext/#parsing-wikitext","title":"Parsing Wikitext","text":"<p>First, we need to import <code>mwparserfromhell</code>. Then, we use the <code>parse</code> function and pass in our wikitext, which will return a <code>Wikicode</code> object.</p> SourceResult Python<pre><code>parsed = mwparserfromhell.parse(wikitext)\nprint(type(parsed)) # &lt;class 'mwparserfromhell.wikicode.Wikicode'&gt;\n</code></pre> Python Console Session<pre><code>&lt;class 'mwparserfromhell.wikicode.Wikicode'&gt;\n</code></pre>"},{"location":"Parsing%20Wikitext/Parsing%20Wikitext/#retrieving-headings-and-sectioning","title":"Retrieving Headings and Sectioning","text":"<p><code>Wikicode</code> objects have several capabilities. You can obtain lists of different components of the wikitext by using a set of <code>filter</code> methods. For instance, you can use the <code>filter_headings</code> method to retrieve a list of all headings as follows:</p> SourceResult Python<pre><code>headings = parsed.filter_headings()\nprint(type(headings)) # &lt;class 'list'&gt;\nprint(len(headings)) # 11\n</code></pre> Python Console Session<pre><code>&lt;class 'list'&gt;\n11\n</code></pre> <p>We found 11 headings. Each element of the list is a <code>Heading</code> object. We can extract the <code>title</code> and the <code>level</code> of the heading objects.</p> <p>Let us inspect the first heading in the list:</p> SourceResult Python<pre><code>heading = headings[0]\n\nprint(f'{heading = }\\n{type(heading) = }\\n')\n\nprint(f'{heading.level = }\\n{type(heading.level) = }\\n')\n\nprint(f'{heading.title = }\\n{type(heading.title) = }\\n')\n</code></pre> Python Console Session<pre><code>heading = '== stark ({{Sprache|Deutsch}}) =='\ntype(heading) = &lt;class 'mwparserfromhell.nodes.heading.Heading'&gt;\n\nheading.level = 2\ntype(heading.level) = &lt;class 'int'&gt;\n\nheading.title = ' stark ({{Sprache|Deutsch}}) '\ntype(heading.title) = &lt;class 'mwparserfromhell.wikicode.Wikicode'&gt;\n</code></pre> <p>The first heading is a second-level heading with the title <code>stark ({{Sprache|Deutsch}})</code>. Notice that we did not find any first-level headings. This is because the first-level heading is reserved for internal use, and wiki contributors can only begin creating headings from the second level onwards.</p> <p>Let us use this information to create a helper function that prints the headings tree of the text.</p> Python<pre><code>def print_headings_tree(parsed):\n    headings = parsed.filter_headings()\n    for heading in headings:\n        print(' ' * 5 *(heading.level - 2), heading.level, heading.title)\n</code></pre> SourceResult Python<pre><code>print_headings_tree(parsed)\n</code></pre> Python Console Session<pre><code> 2  stark ({{Sprache|Deutsch}}) \n      3  {{Wortart|Adjektiv|Deutsch}} \n           4  {{\u00dcbersetzungen}} \n 2  stark ({{Sprache|Englisch}}) \n      3  {{Wortart|Adjektiv|Englisch}} \n           4  {{\u00dcbersetzungen}} \n      3  {{Wortart|Adverb|Englisch}} \n           4  {{\u00dcbersetzungen}} \n 2  stark ({{Sprache|Schwedisch}}) \n      3  {{Wortart|Adjektiv|Schwedisch}} \n           4  {{\u00dcbersetzungen}} \n</code></pre> <ul> <li> <p>The first thing to notice when looking at the second-level headings is that the <code>Sprache</code> includes not only <code>Deutsch</code>, but also <code>Englisch</code> and <code>Schwedisch</code>.</p> <ul> <li>Indeed, wiktionaries are multilingual. Thus, the German Wiktionary does not only cover information related to the German-to-German dictionary, but also includes bilingual dictionaries for several languages to German, such as English to German in this example.</li> </ul> </li> <li> <p>Notice as well that the third-level headings refer to the word forms (<code>Wortart</code>). A word can have one or more word forms. For instance, in English, <code>stark</code> is both an adjective and an adverb.</p> </li> <li> <p>Finally, the fourth-level headings contain information on the translation of the word into different languages (<code>\u00dcbersetzungen</code>).</p> </li> </ul> <p>For my project, I only need the German-to-German dictionary. So, let us extract the wikitext for that heading. We can use the method <code>get_sections()</code>, which accepts a heading level as an argument. Passing level 2 will split the text into sections based on the second-level headings.</p> SourceResult Python<pre><code>sections = parsed.get_sections(levels=[2])\nprint(len(sections)) # 3\n</code></pre> Python Console Session<pre><code>3\n</code></pre> <p>We obtained a list of 3 sections, one for each of the languages.</p> <p>We could use <code>sections_DE = sections.pop(0)</code> to obtain the German section. Alternatively, we can use the <code>matches</code> parameter to retrieve sections whose heading title matches <code>Deutsch</code>, as follows:</p> SourceResult Python<pre><code>sections_DE = parsed.get_sections(levels=[2], matches=\"Deutsch\")\n\n# get_section() retreives always a list!\nprint(type(sections_DE)) # &lt;class 'list'&gt;\nprint(len(sections_DE)) # 1\n\n# Get the first element.\nsections_DE = sections_DE[0]\nprint(type(sections_DE)) # &lt;class 'mwparserfromhell.wikicode.Wikicode'&gt;\n</code></pre> Python Console Session<pre><code>&lt;class 'list'&gt;\n1\n&lt;class 'mwparserfromhell.wikicode.Wikicode'&gt;\n</code></pre> <p>Let us have a look inside the German section.</p> SourceResult Python<pre><code>print(sections_DE[:450])\n</code></pre> Python Console Session<pre><code>== stark ({{Sprache|Deutsch}}) ==\n=== {{Wortart|Adjektiv|Deutsch}} ===\n\n{{Deutsch Adjektiv \u00dcbersicht\n|Positiv=stark\n|Komparativ=st\u00e4rker\n|Superlativ=st\u00e4rksten\n|Bild 1=Weight lifting black and white.jpg|mini|1|eine ''starke'' [[Frau]] beim [[Gewichtheben]]\n|Bild 2=Agonis Flexuosa - bark.jpg|mini|3|ein ''starker'' [[Baumstamm]]\n|Bild 3=Snow pile (3123493946).jpg|mini|4|Es hat ''stark'' [[schneien|geschneit]].\n}}\n\n{{Worttrennung}}\n:stark, {{Komp.}} s\n</code></pre> <p>Sections include all of their subheadings by default.</p> <p>SourceResult Python<pre><code>print_headings_tree(sections_DE)\n</code></pre> Python Console Session<pre><code> 2  stark ({{Sprache|Deutsch}}) \n      3  {{Wortart|Adjektiv|Deutsch}} \n           4  {{\u00dcbersetzungen}} \n</code></pre>  If you want to retrieve a section but not its subsections, set the <code>flat</code> parameter to <code>True</code>.</p> <p>For instance, if we want to retrieve <code>Wortart</code> sections (level 3) without including the translation sections <code>\u00dcbersetzungen</code> (level 4), we can use the following code:</p> SourceResult Python<pre><code># flat = True\nwordforms_DE = sections_DE.get_sections(levels=[3]\n                                        , matches=\"Wortart\"\n                                        , flat=True)\n\n# Get the first element\nwordforms_DE = wordforms_DE[0]\n\n# Check \nprint_headings_tree(wordforms_DE)\n</code></pre> Python Console Session<pre><code>      3  {{Wortart|Adjektiv|Deutsch}} \n</code></pre> <p>The most important information one might be interested in extracting is contained in the Wortart section. Here, you can find definitions, example phrases, synonyms, antonyms, rhymes, proverbs, translations, and more.</p> <p>Since most of this information is related in one way or another to templates, let us learn how to use <code>mwparserfromhell</code> to extract wiki templates.</p>"},{"location":"Parsing%20Wikitext/Parsing%20Wikitext/#extracting-templates","title":"Extracting Templates","text":"<p>You can use the <code>filter_templates</code> method to obtain a list of templates. Each element in the returned list will be a <code>Template</code> object.</p> SourceResult Python<pre><code>wordforms_tpls = wordforms_DE.filter_templates()\nprint(len(wordforms_tpls)) # 49\ntpl = wordforms_tpls[0]\nprint(type(tpl)) # &lt;class 'mwparserfromhell.nodes.template.Template'&gt;\n</code></pre> Python Console Session<pre><code>49\n&lt;class 'mwparserfromhell.nodes.template.Template'&gt;\n</code></pre> <p>The wikicode <code>wordforms_DE</code> contains 49 templates. Let us print the first 3 templates.</p> SourceResult Python<pre><code>for tpl in wordforms_tpls[:3]:\n    print(tpl,'\\n')\n</code></pre> Python Console Session<pre><code>{{Wortart|Adjektiv|Deutsch}} \n\n{{Deutsch Adjektiv \u00dcbersicht\n|Positiv=stark\n|Komparativ=st\u00e4rker\n|Superlativ=st\u00e4rksten\n|Bild 1=Weight lifting black and white.jpg|mini|1|eine ''starke'' [[Frau]] beim [[Gewichtheben]]\n|Bild 2=Agonis Flexuosa - bark.jpg|mini|3|ein ''starker'' [[Baumstamm]]\n|Bild 3=Snow pile (3123493946).jpg|mini|4|Es hat ''stark'' [[schneien|geschneit]].\n}} \n\n{{Worttrennung}} \n</code></pre> <p>There are two important templates worth examining: the <code>Wortart</code> template and the <code>\u00dcbersicht</code> template. These are the first and second templates in the template list <code>wordforms_tpls</code>.</p>"},{"location":"Parsing%20Wikitext/Parsing%20Wikitext/#wortart-template","title":"<code>Wortart</code> Template","text":"<p>From the <code>Wortart</code> template, you can extract the Part of Speech (POS), which indicates the type of word, such as nouns, verbs, and adjectives.</p> <p>You can extract this template using <code>wortart_tpl = wortart_tpls[0]</code> or, alternatively, by using the <code>matches</code> parameter.</p> SourceResult Python<pre><code>wortart_tpls = wordforms_DE.filter_templates(matches='Wortart')\nwortart_tpl = wortart_tpls[0]\nprint(wortart_tpl) # {{Wortart|Adjektiv|Deutsch}}\n</code></pre> Python Console Session<pre><code>{{Wortart|Adjektiv|Deutsch}}\n</code></pre> <p>Let us get the <code>name</code> and parameters (<code>params</code>) of the template:</p> SourceResult Python<pre><code>print(wortart_tpl.name) # Wortart\nprint(wortart_tpl.params) # ['Adjektiv', 'Deutsch']\nprint(type(wortart_tpl.params)) # &lt;class 'list'&gt;\n</code></pre> Python Console Session<pre><code>Wortart\n['Adjektiv', 'Deutsch']\n&lt;class 'list'&gt;\n</code></pre> <p>The <code>wortart_tpl</code> template has two unnamed parameters. The first is assigned the name <code>'1'</code> and contains information about the POS of the word (here, <code>'Adjektiv'</code>). The second is assigned the name <code>'2'</code> and relates to the language (here, <code>'Deutsch'</code>).</p> <p>Since <code>wortart_tpl.params</code> is simply a list, you can use list indexes to extract any parameter. For instance, you can use <code>wortart_tpl.params[1]</code> to extract the second parameter.</p> <p>You can also extract a parameter by its name using the <code>get</code> method.</p> <p>Let us extract the POS parameter by name (<code>'1'</code>).</p> SourceResult Python<pre><code>pos = wortart_tpl.get('1')\nprint(f'{type(pos) = }\\n')\n\nprint(f'{pos.name = }')\nprint(f'{type(pos.name) = }\\n')\n\nprint(f'{pos.value = }')\nprint(f'{type(pos.value) = }\\n')\n</code></pre> Python Console Session<pre><code>type(pos) = &lt;class 'mwparserfromhell.nodes.extras.parameter.Parameter'&gt;\n\npos.name = '1'\ntype(pos.name) = &lt;class 'mwparserfromhell.wikicode.Wikicode'&gt;\n\npos.value = 'Adjektiv'\ntype(pos.value) = &lt;class 'mwparserfromhell.wikicode.Wikicode'&gt;\n</code></pre> <p>Note that the parameter <code>pos</code> is a <code>Parameter</code> object with <code>name</code> and <code>value</code> attributes. Also, notice that the <code>name</code> and <code>value</code> attributes are not strings but <code>Wikicode</code> objects!</p> <p>If you need the string representation of these objects, you can use the <code>str()</code> function. For instance, let us create a helper function that will transform the parameters of a <code>Template</code> into a dictionary of strings:</p> <p>Python<pre><code>def template_to_dict(template):\n    \"\"\"Get dictionary from template object.\"\"\"\n    params = {str(p.name).strip():str(p.value).strip() \n                for p in template.params}\n    return params\n</code></pre> Now, let us see the result.</p> SourceResult Python<pre><code>params = template_to_dict(wortart_tpl)\nfor key, value in params.items():\n    print(key, '=' , value)\n</code></pre> Python Console Session<pre><code>1 = Adjektiv\n2 = Deutsch\n</code></pre>"},{"location":"Parsing%20Wikitext/Parsing%20Wikitext/#conjugation-or-declension-templates","title":"Conjugation or Declension Templates","text":"<p>The \"conjugation or declension table,\" referred to as \"Flexionstabelle\" in the German Wiktionary, is constructed from templates whose names end with <code>\u00dcbersicht</code>:</p> <ul> <li><code>Deutsch Adjektiv \u00dcbersicht</code></li> <li><code>Deutsch Substantiv \u00dcbersicht</code></li> <li><code>Deutsch Verb \u00dcbersicht</code>, etc.</li> </ul> <p>Therefore, we can obtain these templates by matching the word <code>\u00dcbersicht</code>.</p> SourceResult Python<pre><code>\u00fcbersichts_tpls = wordforms_DE.filter_templates(matches='\u00dcbersicht')\n\n# Check length\nprint(len(\u00fcbersichts_tpls)) # 1\n\n# Extract template\n\u00fcbersichts_tpl = \u00fcbersichts_tpls[0]\n\n# Print name\nprint(f'{\u00fcbersichts_tpl.name = }')\n</code></pre> Python Console Session<pre><code>1\n\u00fcbersichts_tpl.name = 'Deutsch Adjektiv \u00dcbersicht\\n'\n</code></pre> <p>Let us print it using our helper function:</p> SourceResult Python<pre><code>for key, value in template_to_dict(\u00fcbersichts_tpl).items():\n    print(key, '=' , value)\n</code></pre> Python Console Session<pre><code>Positiv = stark\nKomparativ = st\u00e4rker\nSuperlativ = st\u00e4rksten\nBild 1 = Weight lifting black and white.jpg\n1 = mini\n2 = 1\n3 = eine ''starke'' [[Frau]] beim [[Gewichtheben]]\nBild 2 = Agonis Flexuosa - bark.jpg\n4 = mini\n5 = 3\n6 = ein ''starker'' [[Baumstamm]]\nBild 3 = Snow pile (3123493946).jpg\n7 = mini\n8 = 4\n9 = Es hat ''stark'' [[schneien|geschneit]].\n</code></pre> <p>This is more information than I need. I will exclude keys that contain <code>Bild</code> and all unnamed parameters (those with numeric names). These parameters relate to the images and their formatting that appear at the bottom of the declension table on the wiki page.</p> Python<pre><code>\u00fcbersichts_dict = {k:v for k,v in template_to_dict(\u00fcbersichts_tpl).items() \n                if 'Bild' not in k  \n                and not k.isnumeric()}\n</code></pre> <p>Let us see what we have obtained:</p> <p>SourceResult Python<pre><code>for key, value in \u00fcbersichts_dict.items():\n    print(key, '=' , value)\n</code></pre> Python Console Session<pre><code>Positiv = stark\nKomparativ = st\u00e4rker\nSuperlativ = st\u00e4rksten\n</code></pre>  That is exactly what I need. </p>"},{"location":"Parsing%20Wikitext/Parsing%20Wikitext/#extracting-other-important-content","title":"Extracting Other Important Content","text":"<p>Although <code>mwparserfromhell</code> is an excellent tool, not all tasks can be accomplished using only this tool. Sometimes, it is necessary to incorporate regular expressions here and there.</p> <p>If you want to extract, for instance, the <code>Bedeutungen</code> (Meanings), the <code>Beispiele</code> (Phrase Examples), the <code>Synonyme</code> (Synonyms), or the <code>Sprichw\u00f6rter</code> (Proverbs), among others, you may notice that they all follow the same pattern.</p> <p>We can take advantage of this pattern and begin writing the blocks for our regular expression right away.</p> <p>For all of them:</p> <ul> <li>Their content is located in separate paragraphs:<ul> <li>They start and end with two new lines <code>\\n\\n ... \\n\\n</code>.</li> </ul> </li> <li>The first line of the paragraph contains only a template without parameters, whose <code>name</code> indicates the type of content in the paragraph:<ul> <li>For example, <code>{{Bedeutungen}}\\n</code> or <code>{{Beispiele}}\\n</code>.</li> <li>Note that <code>{</code> are special characters in regular expressions, so we must escape them using <code>\\</code>:<ul> <li><code>\\{\\{Bedeutungen\\}\\}\\n</code>.</li> </ul> </li> </ul> </li> <li>From the second line onward, the content we want to extract begins:<ul> <li>It includes at least one or more characters <code>.+</code>.</li> <li>It can span several lines.<ul> <li>Note that <code>.</code> matches any character except new lines. Since we want the content to span multiple lines, we need to add the flag <code>re.DOTALL</code> so that <code>.</code> can match new lines (<code>\\n</code>) as well.</li> </ul> </li> <li>It should stop when a new paragraph starts:<ul> <li>To ensure this, we add the non-greedy qualifier <code>?</code>, making it <code>.+?</code>.</li> </ul> </li> <li>Finally, we want to retrieve the content starting from the second line onward:<ul> <li>To do this, we create a capturing group by enclosing it in parentheses:<ul> <li><code>(.+?)</code>.</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>Putting everything together, we get the following pattern:</p> <p><code>r'\\n\\n\\{\\{Bedeutungen\\}\\}\\n(.+?)\\n\\n'</code>, which should be used with the <code>re.DOTALL</code> flag. </p> <p>Let us try it using the <code>re.search</code> method:</p> SourceResult Python<pre><code># Define the pattern \npattern = r'\\n\\n\\{\\{Bedeutungen\\}\\}\\n(.+?)\\n\\n'\n\n# Get your text (convert Wikicode to string)\ntext = str(wordforms_DE)  \n\n# Perform the search\nsearch = re.search(pattern, text, re.DOTALL)\n\n# Extract the content from the capturing group\nbedeutungen_text = search.group(1)\n\n# Print some characters of the text\nprint(bedeutungen_text[:300])\n</code></pre> Python Console Session<pre><code>:[1] mit [[Kraft]] [[ausgestattet]], von Kraft [[pr\u00e4gen|gepr\u00e4gt]], [[zeugen]]d\n:[2] [[hoch|hohe]] [[Leistung]] [[erbringen]]d; sehr [[leistungsf\u00e4hig]]\n:[3] [[\u00e4u\u00dfer-|\u00e4u\u00dferen]] [[Einfluss|Einfl\u00fcssen]], [[Belastung]]en [[standhalten]]d (aufgrund von [[Gr\u00f6\u00dfe]], [[Dicke]] oder \u00c4hnlichem)\n:[4] {{K|als [[V\n</code></pre> <p>We have obtained the content of <code>Bedeutungen</code>, but it is difficult to read because it contains many Wiki links <code>[[link]]</code> and indentation syntax <code>:</code>.</p> <p>To improve readability, we can use the <code>strip_code</code> method from <code>mwparserfromhell</code> to convert the Wikicode into plain text.</p> SourceResult Python<pre><code># Parse the found text into Wikicode\nbedeutungen = mwparserfromhell.parse(bedeutungen_text)\n\n# Strip Wiki links, templates, and any special Wiki code\nbedeutungen_plain = bedeutungen.strip_code()\n\n# Print the start of the text \nprint(bedeutungen_plain[:300])\n</code></pre> Python Console Session<pre><code>[1] mit Kraft ausgestattet, von Kraft gepr\u00e4gt, zeugend\n[2] hohe Leistung erbringend; sehr leistungsf\u00e4hig\n[3] \u00e4u\u00dferen Einfl\u00fcssen, Belastungen standhaltend (aufgrund von Gr\u00f6\u00dfe, Dicke oder \u00c4hnlichem)\n[4]  sehr, in hohem Ma\u00dfe; intensiv\n[5] reich an bestimmten Stoffen, Inhalt\n[6] von Macht, Einfluss gepr\n</code></pre> <p>Finally, we will wrap this code into a function, allowing us to parse any content type by providing the content's <code>name</code>. We will also include an option to retrieve plain text when the <code>strip_code</code> option is set to <code>True</code>.</p> Python<pre><code>def content_extract(name, text, strip_code=True):\n    # Adjust the pattern to accept the content name\n    pattern = r'\\n\\n\\{\\{' + name + r'\\}\\}\\n(.+?)\\n\\n'\n    search = re.search(pattern, text, re.DOTALL)\n    content = search.group(1)\n    # Return plain text if strip_code is True\n    if strip_code:\n        return mwparserfromhell.parse(content).strip_code()\n    # Return raw wikitext if strip_code is False\n    else:\n        return content\n</code></pre> <p>Let us try it using the content types: 'Bedeutungen', 'Beispiele', 'Synonyme', 'Sprichw\u00f6rter' and print the first 150 characters of each content.</p> SourceResult Python<pre><code># Get your text (convert Wikicode to string)\ntext = str(wordforms_DE) \n\n# Get different content types\nfor name in ['Bedeutungen', 'Beispiele', 'Synonyme', 'Sprichw\u00f6rter']:\n    print(name.center(20, '-'))\n    content = content_extract(name, text)\n    print(content[:150], '\\n')\n</code></pre> Python Console Session<pre><code>----Bedeutungen-----\n[1] mit Kraft ausgestattet, von Kraft gepr\u00e4gt, zeugend\n[2] hohe Leistung erbringend; sehr leistungsf\u00e4hig\n[3] \u00e4u\u00dferen Einfl\u00fcssen, Belastungen standhalt \n\n-----Beispiele------\n[1] Er hat viele Muskeln \u2013 er ist stark.\n[1] Es weht ein starker Wind.\n[1] Ein starker Mann kann schwere Sachen tragen.\n[1] \u201eIndiz f\u00fcr den verbreitete \n\n------Synonyme------\n[1] kr\u00e4ftig, kraftvoll\n[2] effizient, leistungsf\u00e4hig, leistungsstark, wirksam\n[3] belastbar, dick, fest, robust, stabil, widerstandsf\u00e4hig\n[4] ausgepr\u00e4 \n\n----Sprichw\u00f6rter----\nWas dich nicht umbringt, macht dich st\u00e4rker\nDer Starke ist am m\u00e4chtigsten allein.Friedrich Schiller: Wilhelm Tell (1804) \n</code></pre> <p>What a fitting proverb (Sprichwort) to conclude this tutorial\u2014it was not planned at all!  </p> <p>Was dich nicht umbringt, macht dich st\u00e4rker </p> <p>A reminder that:  </p> <p>What does not kill you makes you stronger. </p> <p>We have now reached the end of this hands-on guide!  </p> <p>You can use this code to create your own DerDieDas game or to perform linguistic analyses.  </p> <p>If you are interested, feel free to explore the <code>de_wiktio</code> project, which implements all the steps covered in this tutorial in one package.  </p>"},{"location":"Parsing%20Wikitext/Wiki%20Basics/","title":"Wiki Basics","text":"On this page<ul> <li>Wikitext Syntax</li> <li>Wiki Namespaces</li> <li>Wiki Templates<ul> <li>Examples of Templates</li> </ul> </li> </ul>"},{"location":"Parsing%20Wikitext/Wiki%20Basics/#wikitext-syntax","title":"Wikitext Syntax","text":"<p>Wikitext, also known as wikicode, is a lightweight markup language used across Wikimedia projects, including Wikipedia and the German Wiktionary, to format and organize content. Wikitext provides special syntax for adding links, formatting text, creating section headings, and more. A key feature of wikitext is its use of templates, which help standardize and structure the content.</p> <p>\"Wiki contributors\" write their entries in wikitext, which is then processed and displayed as a formatted wikipage.</p> <p>Below, some examples of wikitext syntax, for a more detailed overview  of the wikitext syntax here and more examples here.</p> <p>Examples of wikitext syntax:</p> <ul> <li>Headings: Created using equal signs (<code>=</code>), with the number of equal signs on either side of the heading indicating its level. Wikitext<pre><code>= Main Heading =\n== Subheading ==\n</code></pre></li> <li>Templates: Defined within double curly braces (<code>{{ }}</code>)  Wikitext<pre><code>{{template_name|non-named-parameter|named-param=value}}\n</code></pre></li> <li>Internal Links: Formatted with double square brackets (<code>[[ ]]</code>).</li> <li>Bold Text: Indicated with triple quotes (<code>'''bold'''</code>).</li> <li>Italics: Indicated with double quotes (<code>''italics''</code>).</li> </ul> <p>Although regular expressions can be used to parse wikitext, doing so can be quite challenging. This is especially true because wikitext allows for nested templates and uses quotation marks for both bold and italic text, among other formatting features.</p> <p>Fortunately, there are some Python packages available for parsing wikitext. For a full list of parsers available in different languages, visit this wiki page.</p> <p>In this tutorial, we will use the package <code>mwparserfromhell</code>. You can check out the project site on GitHub here and view its documentation page here.</p> <p>Although I have not had the chance to use it, another Python wiki parser that may be worth exploring is <code>wikitextparser</code>.</p>"},{"location":"Parsing%20Wikitext/Wiki%20Basics/#wiki-namespaces","title":"Wiki Namespaces","text":"<p>Wikimedia projects use namespaces to organize and categorize pages based on their purpose, such as main content pages, templates, help pages, and discussion pages.</p> <p>Below is a list of some important namespaces in the German Wiktionary.  For the full list, see here.</p> ID Description Prefix English Prefix German Countin Wiktionary 0 The Main namespace, or default namespace, contains the actual content.- word entries of dictionary section - - 1,142,612 pages 108 For Flexion pages. Flexion Flexion 67,542 106 For List of rhyming words. Reim Reim 48,028 10 For Template/Vorlage pages. Template Vorlage 7,258 12 For Help/Hilfe resources. Help Hilfe 588 <p>Namespaces have an ID (such as <code>'0'</code>, <code>'108'</code>) and prefixes. The prefixes are part of the page titles and help with navigating the site. The only namespace without a prefix is the main content namespace (<code>'0'</code>).</p> <p>For instance, if you are interested in information about the word sch\u00f6n, you can visit the main content page for sch\u00f6n (without a prefix) or its declension table, which can be found under Flexion:sch\u00f6n (with the Flexion prefix).</p>"},{"location":"Parsing%20Wikitext/Wiki%20Basics/#wiki-templates","title":"Wiki Templates","text":"<p>Templates (or Vorlage in German) can be thought of as small functions. They have a name, as well as named and unnamed parameters, which are separated by pipes.</p> <p>The first, second, third, etc., unnamed parameters are assigned the names <code>1</code>, <code>2</code>, <code>3</code>, and so on.</p> <p>Templates have their own namespace in wiki projects (<code>'10'</code>), with the prefix <code>Template</code> in English and <code>Vorlage</code> in German. This means that if you need to read the documentation for a template in the German Wiktionary, you can simply search for the page titled <code>Vorlage:name_of_template</code>.</p> <p>Below are some examples of templates, including the wikicode, their end result on the web page, and their documentation page.</p>"},{"location":"Parsing%20Wikitext/Wiki%20Basics/#examples-of-templates","title":"Examples of Templates","text":"<p>For instance, the template named <code>Wort der Woche</code> has two unnamed parameters.</p> TemplateWebTemplate's docs Wikitext<pre><code>{{Wort der Woche|26|2007}}\n</code></pre> <p></p> <p>Wort der Woche - Template docs </p> <p>The template <code>Deutsch Adjektiv \u00dcbersicht</code> generates the declension tables for adjectives. In this example, it has three named parameters: <code>Positiv</code>, <code>Komparativ</code>, and <code>Superlativ</code>.</p> TemplateWebTemplate's docs Wikitext<pre><code>{{Deutsch Adjektiv \u00dcbersicht\n|Positiv=sch\u00f6n\n|Komparativ=sch\u00f6ner\n|Superlativ=sch\u00f6nsten\n}}\n</code></pre> <p></p> <p>Deutsch Adjektiv \u00dcbersicht - Template docs </p> <p>Different word forms rely on different templates to generate the inflection tables. For instance, verbs use the <code>Deutsch Verb \u00dcbersicht</code> template.</p> TemplateWebTemplate's docs Wikitext<pre><code>{{Deutsch Verb \u00dcbersicht\n|Pr\u00e4sens_ich=arbeite\n|Pr\u00e4sens_du=arbeitest\n|Pr\u00e4sens_er, sie, es=arbeitet\n|Pr\u00e4teritum_ich=arbeitete\n|Partizip II=gearbeitet\n|Konjunktiv II_ich=arbeitete\n|Imperativ Singular=arbeite\n|Imperativ Plural=arbeitet\n|Hilfsverb=haben\n}}\n</code></pre> <p></p> <p>Deutsch Verb \u00dcbersicht - Template docs </p> <p>Templates can hide large amounts of code behind the scenes; for example, this small template produces a large table.</p> TemplateWebTemplate's docs Wikitext<pre><code>{{Deklinationsseite Adjektiv\n|Positiv-Stamm=sch\u00f6n\n|Komparativ-Stamm=sch\u00f6ner\n|Superlativ-Stamm=sch\u00f6nst\n}}\n</code></pre> <p></p> <p>Deklinationsseite Adjektiv - Template docs </p>"},{"location":"Parsing%20XML/","title":"Parsing XML","text":"<p>In this section, we will explore how to use the <code>lxml</code> package to parse XML content from Wiki sites. Our goal is to extract the Wikitext, which will be processed in the next section.</p> <p>How we parse the XML content depends on how we retrieve it:</p> <ul> <li>Whether using the Special Export tool</li> <li>Or from the Dump File</li> </ul> <p>Let us begin by parsing XML content retrieved using the Special Export tool.</p>"},{"location":"Parsing%20XML/Parsing%20XML%20from%20Dump%20file/","title":"From Dump file","text":"On this page<ul> <li>Importing Packages</li> <li>Setting Up Paths Local Machine</li> <li>Parsing the XML File</li> <li>Displaying the XML Structure</li> <li>Extracting Data<ul> <li>element.findall</li> </ul> </li> <li>Saving the Dictionary Locally</li> <li>Loading Dictionary</li> </ul>"},{"location":"Parsing%20XML/Parsing%20XML%20from%20Dump%20file/#importing-packages","title":"Importing Packages","text":"Python<pre><code>from pathlib import Path \nimport lxml.etree as ET # to parse XML documents\nimport pickle # to store the dictionary locally \n</code></pre>"},{"location":"Parsing%20XML/Parsing%20XML%20from%20Dump%20file/#setting-up-paths-local-machine","title":"Setting Up Paths Local Machine","text":"<p>To follow along in this section:</p> <ol> <li>You will need to download and decompress the Wiktionary dump file.  <ul> <li>You can download the latest version here or refer to instructions for downloading specific versions here.</li> </ul> </li> <li>Once you have done that, specify the path to the decompressed file in <code>XML_FILE</code>.</li> <li>By the end of this section, we will save our result as a dictionary and store it locally.  <ul> <li>Therefore, do not forget to specify in which folder the dictionary should be saved in <code>DICT_PATH</code>.</li> </ul> </li> </ol> Python<pre><code># Specify your own paths\nXML_FILE = Path(r'path\\to\\xml\\dewiktionary-20241020-pages-articles-multistream.xml')\nDICT_PATH = Path(r'path\\to\\dict')\n</code></pre>"},{"location":"Parsing%20XML/Parsing%20XML%20from%20Dump%20file/#parsing-the-xml-file","title":"Parsing the XML File","text":"<p>Since we are working with a file, we cannot use the <code>ET.fromstring</code> function to parse the XML content. Instead, we must use the <code>ET.parse</code> function.</p> <p>Note that this process can take some time. On my computer, it takes approximately 42 seconds to load the entire XML tree.</p> SourceResult Python<pre><code># ET.parse for a xml file\ntree = ET.parse(XML_FILE)\nprint(type(tree)) # lxml.etree._ElementTree\n\nroot = tree.getroot()\nprint(type(root)) # &lt;class 'lxml.etree._Element'&gt;\n</code></pre> Python Console Session<pre><code>&lt;class 'lxml.etree._ElementTree'&gt;\n&lt;class 'lxml.etree._Element'&gt;\n</code></pre> <p>The parser returns an <code>ElementTree</code> object. We use the <code>getroot()</code> method to access the root <code>Element</code>.</p>"},{"location":"Parsing%20XML/Parsing%20XML%20from%20Dump%20file/#displaying-the-xml-structure","title":"Displaying the XML Structure","text":"<p>The XML structure of the dump file is quite large, so printing the entire tree would not only be inefficient but also quite overwhelming. To make it more manageable, let us modify our <code>print_tags_tree</code> function.</p> <p>We will add options to limit the number of children displayed for the root element and to control the depth of the tree.</p> <p>Here is our updated <code>print_tags_tree</code> function:</p> Python<pre><code>def print_tags_tree(elem, level=0, only_tagnames=False, max_children=5, max_level=5):\n\n    tagname = ET.QName(elem).localname if only_tagnames else elem.tag\n    print(\" \" * 5 * level, level, tagname)\n\n    # Restrict depth\n    if level + 1 &lt;= max_level:\n        for child_index, child in enumerate(elem):\n            print_tags_tree(child, level + 1, only_tagnames, max_children, max_level)\n            # Limit number of children of the root element\n            if level == 0 and child_index == max_children - 1:\n                break\n</code></pre> <p>To display only the first 5 direct children of the root element and limit the tree to the first level:</p> SourceResult Python<pre><code>print_tags_tree(root, only_tagnames=True, max_children=5, max_level=1)\n</code></pre> Python Console Session<pre><code>0 mediawiki\n    1 siteinfo\n    1 page\n    1 page\n    1 page\n    1 page\n</code></pre> <p>To view the first 3 children of the root element and display two levels of the tree:</p> SourceResult Python<pre><code>print_tags_tree(root, only_tagnames=True, max_children=3, max_level=2)\n</code></pre> Python Console Session<pre><code>0 mediawiki\n    1 siteinfo\n        2 sitename\n        2 dbname\n        2 base\n        2 generator\n        2 case\n        2 namespaces\n    1 page\n        2 title\n        2 ns\n        2 id\n        2 revision\n    1 page\n        2 title\n        2 ns\n        2 id\n        2 revision\n</code></pre>"},{"location":"Parsing%20XML/Parsing%20XML%20from%20Dump%20file/#extracting-data","title":"Extracting Data","text":""},{"location":"Parsing%20XML/Parsing%20XML%20from%20Dump%20file/#elementfindall","title":"<code>element.findall</code>","text":"<p>As with the previous section, we are interested in extracting the <code>page</code>, <code>title</code>, <code>ns</code>, and <code>text</code> tags.</p> <p>The main difference in structure here is that we now have multiple <code>page</code> elements, and we want to extract all of them.</p> <p>We cannot use <code>find</code>, because it will return only the first <code>page</code>. However, we can use the <code>findall</code> method instead, which will return a list of all <code>page</code> elements.</p> SourceResult Python<pre><code>NAMESPACES = root.nsmap \npages = root.findall('page', namespaces=NAMESPACES)\nprint(len(pages)) # as of today 1281638\n</code></pre> Python Console Session<pre><code>1281638\n</code></pre> <p>Notice that the latest dump file version contains more than one million pages, and retrieving them all takes approximately 45 seconds.</p> <p>Since retrieving all pages is time-consuming, we will store the relevant information locally in a dictionary and save it as a pickle file for quicker access in the future.</p> <p>We will create a dictionary, <code>dict_0</code>, using page titles as keys and their wikitext as values. Additionally, we will restrict the pages we store to those within the main Wiki namespace (<code>'0'</code>). We will discuss Wiki namespaces further when we parse wikitext.</p> <p>This process may take a couple of minutes!</p> Python<pre><code>ns = '0'\ndict_0 = dict()\nfor page in pages:\n    ns_elem = page.find('ns', namespaces=NAMESPACES)\n    if ns_elem.text == ns: \n        title = page.find('title', namespaces=NAMESPACES)\n        wikitext = page.find('revision/text', namespaces=NAMESPACES)\n        dict_0[title.text] = wikitext.text\n</code></pre> <p>To check that our dictionary is correctly populated, let us print out part of the wikitext for a sample page:</p> SourceResult Python<pre><code>print(dict_0['sch\u00f6n'][:300])\n</code></pre> Python Console Session<pre><code>{{Siehe auch|[[schon]]}}\n{{Wort der Woche|26|2007}}\n== sch\u00f6n ({{Sprache|Deutsch}}) ==\n=== {{Wortart|Adjektiv|Deutsch}} ===\n\n{{Deutsch Adjektiv \u00dcbersicht\n|Positiv=sch\u00f6n\n|Komparativ=sch\u00f6ner\n|Superlativ=sch\u00f6nsten\n|Bild 1=Jaguar E-type (serie III).jpg|mini|1|ein ''sch\u00f6nes'' [[Auto]]\n|Bild 2=12er Anitra\n</code></pre>"},{"location":"Parsing%20XML/Parsing%20XML%20from%20Dump%20file/#saving-the-dictionary-locally","title":"Saving the Dictionary Locally","text":"<p>Once the dictionary is built, we save it locally using the <code>pickle</code> module, which allows us to store the dictionary in a serialized format. This way, we will not need to parse the XML file again in the future.</p> Python<pre><code>dict_file = DICT_PATH / f'wikidict_{ns}.pkl'\n\nwith open(dict_file, 'wb') as f:\n    pickle.dump(dict_0, f)\n</code></pre>"},{"location":"Parsing%20XML/Parsing%20XML%20from%20Dump%20file/#loading-dictionary","title":"Loading Dictionary","text":"<p>The next time you need to retrieve wikitext, simply load the dictionary from the pickle file and select the title page you need!</p> SourceResult Python<pre><code>import pickle\nfrom pathlib import Path\n\nns = '0'\ndict_file = DICT_PATH / f'wikidict_{ns}.pkl'\n\nwith open(dict_file, 'rb') as f:\n    dict_0 = pickle.load(f)\n# 9 secs\n\nwikitext = dict_0['sch\u00f6n']\nprint(wikitext[:300])\n</code></pre> Python Console Session<pre><code>{{Siehe auch|[[schon]]}}\n{{Wort der Woche|26|2007}}\n== sch\u00f6n ({{Sprache|Deutsch}}) ==\n=== {{Wortart|Adjektiv|Deutsch}} ===\n\n{{Deutsch Adjektiv \u00dcbersicht\n|Positiv=sch\u00f6n\n|Komparativ=sch\u00f6ner\n|Superlativ=sch\u00f6nsten\n|Bild 1=Jaguar E-type (serie III).jpg|mini|1|ein ''sch\u00f6nes'' [[Auto]]\n|Bild 2=12er Anitra\n</code></pre> <p>And we are done! Now we can retrieve any wikitext by the page title. Next, we can cover how to parse wikitext.</p>"},{"location":"Parsing%20XML/Parsing%20XML%20from%20Special%20Export/","title":"From Special Export","text":"On this page<ul> <li>Importing Packages</li> <li>Parsing the XML content</li> <li>Displaying XML Structure</li> <li>XML Namespaces Overview</li> <li>Extracting Data<ul> <li>element.find</li> </ul> </li> </ul>"},{"location":"Parsing%20XML/Parsing%20XML%20from%20Special%20Export/#importing-packages","title":"Importing Packages","text":"Python<pre><code>import requests # to fetch info from URLs\nimport lxml.etree as ET # to parse XML documents\n</code></pre> <p>We will use the <code>fetch</code> function as described in our earlier tutorial on Special Exports, provided here for reference.</p> Python<pre><code>def fetch(title):\n    url = f'https://de.wiktionary.org/wiki/Spezial:Exportieren/{title}'\n    headers = {\n        \"User-Agent\": \"Search for German words (https://lennon-c.github.io/python-wikitext-parser-guide)\"\n    }\n    resp = requests.get(url, headers=headers)\n    resp.raise_for_status()\n    return resp.text\n</code></pre> <p>Let us fetch the XML content for the page titled <code>'sch\u00f6n'</code> as an example. </p> SourceResult Python<pre><code>xml_content = fetch('sch\u00f6n')\nprint(xml_content[:500])\nprint(f'{type(xml_content) = }')\n</code></pre> XML<pre><code>&lt;mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.11/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.11/ http://www.mediawiki.org/xml/export-0.11.xsd\" version=\"0.11\" xml:lang=\"de\"&gt;\n  &lt;siteinfo&gt;\n    &lt;sitename&gt;Wiktionary&lt;/sitename&gt;\n    &lt;dbname&gt;dewiktionary&lt;/dbname&gt;\n    &lt;base&gt;https://de.wiktionary.org/wiki/Wiktionary:Hauptseite&lt;/base&gt;\n    &lt;generator&gt;MediaWiki 1.46.0-wmf.7&lt;/generator&gt;\n    &lt;case&gt;case-sensitive&lt;/case&gt;\n    &lt;namespa\ntype(xml_content) = &lt;class 'str'&gt;\n</code></pre>"},{"location":"Parsing%20XML/Parsing%20XML%20from%20Special%20Export/#parsing-the-xml-content","title":"Parsing the XML content","text":"<p>Now that we have retrieved the XML content, we will use <code>lxml.etree</code> to parse it. </p> <p>In order to parse an XML string, which is what <code>fetch</code> returns, we will use the  <code>fromstring</code> method. Later we will use the <code>parse</code> method to parse an XML file.</p> SourceResult Python<pre><code># Parse the XML content into an ET Element\nroot = ET.fromstring(xml_content)\n\nprint(type(root)) # Output: &lt;class 'lxml.etree._Element'&gt;\n</code></pre> Python Console Session<pre><code>&lt;class 'lxml.etree._Element'&gt;\n</code></pre> <p><code>ET.fromstring</code> returns an <code>Element</code> object with several useful properties. From an <code>Element</code> object, you can extract:</p> <ul> <li>its tag <code>&lt;tag_name&gt; ... &lt;/tag_name&gt;</code>, using <code>Element.tag</code></li> <li>its attributes <code>&lt;tag_name attribut1=\"value1\" attrib2=\"value2\"&gt; ...</code>, using <code>Element.attrib</code></li> <li>its text <code>&lt;tag_name attribut1=\"value1\"&gt; some text &lt;/tag_name&gt;</code>, using <code>Element.text</code></li> </ul> <p>Let us create a dummy XML content to illustrate these:</p> SourceResult Python<pre><code>xml = \"\"\"\n&lt;tag_name attribut1=\"value1\" attrib2=\"value2\"&gt; some text &lt;/tag_name&gt;\n\"\"\"\n\nelement = ET.fromstring(xml)\n\nprint('tag'.center(20, '*'))\nprint(f'{element.tag = }')\nprint(f'{type(element.tag) = }')\n\nprint('attrib'.center(20, '*'))\nprint(f'{element.attrib = }')\nprint(f'{type(element.attrib) = }')\n\nprint('text'.center(20, '*'))\nprint(f'{element.text = }')\nprint(f'{type(element.text) = }')\n</code></pre> Python Console Session<pre><code>********tag*********\nelement.tag = 'tag_name'\ntype(element.tag) = &lt;class 'str'&gt;\n*******attrib*******\nelement.attrib = {'attribut1': 'value1', 'attrib2': 'value2'}\ntype(element.attrib) = &lt;class 'lxml.etree._Attrib'&gt;\n********text********\nelement.text = ' some text '\ntype(element.text) = &lt;class 'str'&gt;\n</code></pre>"},{"location":"Parsing%20XML/Parsing%20XML%20from%20Special%20Export/#displaying-xml-structure","title":"Displaying XML Structure","text":"<p>XML data can often be large and complex, especially when deeply nested, which makes understanding its structure difficult.</p> <p>To help with this, let us create a helper function to display the XML tags in a tree-like format. Since we do not know how deep the XML structure might go, the best strategy here is to use recursion as follows:</p> Python<pre><code>def print_tags_tree(elem, level=0):\n    # print indent, level and tag of the element\n    print(' ' * 5 * level, level, elem.tag)\n    for child in elem:\n        # recursion to go as deep as possible\n        print_tags_tree(child, level + 1)\n</code></pre> <p>Let us try it: </p> SourceResult Python<pre><code>print_tags_tree(root)\n</code></pre> Python Console Session<pre><code> 0 {http://www.mediawiki.org/xml/export-0.11/}mediawiki\n      1 {http://www.mediawiki.org/xml/export-0.11/}siteinfo\n           2 {http://www.mediawiki.org/xml/export-0.11/}sitename\n           2 {http://www.mediawiki.org/xml/export-0.11/}dbname\n           2 {http://www.mediawiki.org/xml/export-0.11/}base\n           2 {http://www.mediawiki.org/xml/export-0.11/}generator\n           2 {http://www.mediawiki.org/xml/export-0.11/}case\n           2 {http://www.mediawiki.org/xml/export-0.11/}namespaces\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n                3 {http://www.mediawiki.org/xml/export-0.11/}namespace\n      1 {http://www.mediawiki.org/xml/export-0.11/}page\n           2 {http://www.mediawiki.org/xml/export-0.11/}title\n           2 {http://www.mediawiki.org/xml/export-0.11/}ns\n           2 {http://www.mediawiki.org/xml/export-0.11/}id\n           2 {http://www.mediawiki.org/xml/export-0.11/}revision\n                3 {http://www.mediawiki.org/xml/export-0.11/}id\n                3 {http://www.mediawiki.org/xml/export-0.11/}parentid\n                3 {http://www.mediawiki.org/xml/export-0.11/}timestamp\n                3 {http://www.mediawiki.org/xml/export-0.11/}contributor\n                     4 {http://www.mediawiki.org/xml/export-0.11/}username\n                     4 {http://www.mediawiki.org/xml/export-0.11/}id\n                3 {http://www.mediawiki.org/xml/export-0.11/}comment\n                3 {http://www.mediawiki.org/xml/export-0.11/}origin\n                3 {http://www.mediawiki.org/xml/export-0.11/}model\n                3 {http://www.mediawiki.org/xml/export-0.11/}format\n                3 {http://www.mediawiki.org/xml/export-0.11/}text\n                3 {http://www.mediawiki.org/xml/export-0.11/}sha1\n</code></pre> <p>The output of <code>print_tags_tree</code> shows tags in a format that combines:</p> <ul> <li>The namespace URI (e.g., <code>{http://www.mediawiki.org/xml/export-0.11/}</code>)</li> <li>The tag name (e.g., <code>mediawiki</code>, <code>page</code>)</li> </ul> <p>Although knowing the namespace is important, as we will discover later, it makes the tree look very cluttered.</p> <p>To address this, let us modify the helper function to allow printing only the tag names in the tree.</p> <p>We will use the function <code>QName</code>, which splits the tag information of an <code>element</code> into its tag name and its namespace. Here is an example code using <code>QName</code>:</p> SourceResult Python<pre><code>print(f'{root.tag=}')\n\n# Using the ET function QName\nroot_name = ET.QName(root)\n# only tag name\nprint(f'{root_name.localname=}')\n# only namespace\nprint(f'{root_name.namespace=}')\n</code></pre> Python Console Session<pre><code>root.tag='{http://www.mediawiki.org/xml/export-0.11/}mediawiki'\nroot_name.localname='mediawiki'\nroot_name.namespace='http://www.mediawiki.org/xml/export-0.11/'\n</code></pre> <p>Now, let us modify the <code>print_tags_tree</code> function to provide the option of printing only tag names when the <code>only_tagnames</code> parameter is set to <code>True</code>.</p> SourceResult Python<pre><code>def print_tags_tree(elem, level=0, only_tagnames=False):\n    tagname = ET.QName(elem).localname if only_tagnames else elem.tag\n\n    print(' ' * 5 * level, level, tagname)\n    for child in elem:\n        print_tags_tree(child, level + 1, only_tagnames)\n\nprint_tags_tree(root, only_tagnames=True)\n</code></pre> Python Console Session<pre><code> 0 mediawiki\n      1 siteinfo\n           2 sitename\n           2 dbname\n           2 base\n           2 generator\n           2 case\n           2 namespaces\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n                3 namespace\n      1 page\n           2 title\n           2 ns\n           2 id\n           2 revision\n                3 id\n                3 parentid\n                3 timestamp\n                3 contributor\n                     4 username\n                     4 id\n                3 comment\n                3 origin\n                3 model\n                3 format\n                3 text\n                3 sha1\n</code></pre> <p>Better! This gives us a clear view of the structure:</p> <p>The root element is <code>mediawiki</code>.</p> <ul> <li><code>mediawiki</code> has two children:<ul> <li><code>siteinfo</code><ul> <li>Contains information about the domain (e.g., its sitename, Wiki namespace).</li> </ul> </li> <li><code>page</code><ul> <li>Contains the most important information for this project.</li> <li><code>page</code> has four direct children: <code>title</code>, <code>ns</code>, <code>id</code>, and <code>revision</code>.<ul> <li><code>title</code> contains the title of the page<ul> <li>For example, <code>sch\u00f6n</code>, <code>Flexion:sch\u00f6n</code>.</li> </ul> </li> <li><code>ns</code> contains the Wiki namespace, which should not be confused with XML namespaces!<ul> <li>For example, <code>0</code>, <code>108</code>.</li> </ul> </li> <li><code>id</code> is the unique identifier of the page<ul> <li>For example, <code>2930</code>, <code>21734</code>.</li> </ul> </li> <li><code>revision</code> contains a revision of the page.<ul> <li>Each time a wiki page is modified, a <code>revision</code> element is added. In the XML extraction methods covered here, only the latest revision is retrieved, so we have only one <code>revision</code> element.</li> <li>The raw wikitext is located here in the child element <code>text</code>.</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>The main goal of this section is to extract the <code>page</code>, <code>title</code>, <code>ns</code>, and <code>text</code> elements.</p> <p>But first, let us briefly discuss XML namespaces. If you are already familiar with XML namespaces, feel free to skip this part.</p>"},{"location":"Parsing%20XML/Parsing%20XML%20from%20Special%20Export/#xml-namespaces-overview","title":"XML Namespaces Overview","text":"<p>In XML, tag names and attributes are user-defined, which can lead to name conflicts when combining data from different XML files. To avoid these conflicts, XML uses a system of namespaces and prefixes. Each namespace is typically defined using a URI.</p> <p>Namespaces are often declared in the root element of the XML (but not always, they can also be declared in children elements).  To identify namespaces in an XML document, look for attributes beginning with <code>xmlns</code> and/or  <code>xmlns:prefix</code>. </p> <ul> <li><code>xmlns</code> without a prefix: This denotes the default namespace, applying to the element where it appears and all its descendants (unless overridden).</li> <li><code>xmlns:prefix</code>: This is a prefixed namespace. It applies only to elements that explicitly use the prefix.</li> </ul> <p>For example, in our Wiki XML, we see two namespaces defined at the root element:</p> XML<pre><code>&lt;mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.11/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \n</code></pre> <p>This gives us:</p> <ul> <li>Default Namespace: <code>xmlns=\"http://www.mediawiki.org/xml/export-0.11/\"</code>, which applies to all elements without prefixes.</li> <li>Prefixed Namespace: <code>xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"</code> with the prefix <code>xsi</code>.</li> </ul> <p>An even simpler approach is to use the <code>nsmap</code> method, which provides a dictionary mapping prefixes to their respective URIs.</p> SourceResult Python<pre><code>NAMESPACES = root.nsmap\nfor key, namespace in NAMESPACES.items():\n    print('prefix:', key,'=&gt; namespace-URI:', namespace)\n</code></pre> Python Console Session<pre><code>prefix: None =&gt; namespace-URI: http://www.mediawiki.org/xml/export-0.11/\nprefix: xsi =&gt; namespace-URI: http://www.w3.org/2001/XMLSchema-instance\n</code></pre>"},{"location":"Parsing%20XML/Parsing%20XML%20from%20Special%20Export/#extracting-data","title":"Extracting Data","text":""},{"location":"Parsing%20XML/Parsing%20XML%20from%20Special%20Export/#elementfind","title":"<code>element.find</code>","text":"<p>To extract elements from the XML, we will use the <code>find</code> method, which searches for the first child element with a specified tag name or path.</p> <p>Note that the following code will fail to find the <code>page</code> element and will return <code>None</code>. This is because <code>lxml</code> requires the correct namespace to be specified if the XML we are working with has declared any namespaces.</p> SourceResult Python<pre><code># This will not work, because it lacks the required namespace\npage = root.find('page')\nprint(page) # None\n</code></pre> Python Console Session<pre><code>None\n</code></pre> <p>You can specify the namespace in two ways:</p> <ul> <li>Using the full <code>{namespace}tagname</code> notation</li> <li>Passing a namespace dictionary as an argument to <code>find</code></li> </ul> <p>The following code will successfully retrieve the <code>page</code> element using each method:</p> SourceResult Python<pre><code># Full notation \npage = root.find('{http://www.mediawiki.org/xml/export-0.11/}page')\nprint('Full notation:',page) \n\n# Namespace dictionary\n# NAMESPACES = {None: 'http://www.mediawiki.org/xml/export-0.11/'}\nNAMESPACES = root.nsmap\npage = root.find('page', NAMESPACES)\nprint('Namespace dictionary:', page)\n</code></pre> Python Console Session<pre><code>Full notation: &lt;Element {http://www.mediawiki.org/xml/export-0.11/}page at 0x7f89e6a28840&gt;\nNamespace dictionary: &lt;Element {http://www.mediawiki.org/xml/export-0.11/}page at 0x7f89e6a28840&gt;\n</code></pre> <p>We will stick to the namespace dictionary to avoid writing the URL each time we need to use the <code>find</code> method.</p> <p>Now that we have successfully located the <code>page</code> element, we can retrieve its child elements <code>ns</code> and <code>title</code>.</p> SourceResult Python<pre><code># Accessing Wiki namespace\nns = page.find('ns', NAMESPACES)\nprint(ns)\nprint(ns.text) # '0'\n\n# Accessing the title of the page\ntitle = page.find('title', NAMESPACES)\nprint(title)\nprint(title.text) # sch\u00f6n\n</code></pre> Python Console Session<pre><code>&lt;Element {http://www.mediawiki.org/xml/export-0.11/}ns at 0x7f89e6a4bd80&gt;\n0\n&lt;Element {http://www.mediawiki.org/xml/export-0.11/}title at 0x7f89e6a4bd40&gt;\nsch\u00f6n\n</code></pre> <p>Finally, we want to retrieve the main content, or wikitext, which is stored in the <code>text</code> element.</p> <p>Note that we cannot use <code>page.find('text', NAMESPACES)</code> directly because <code>text</code> is not a direct child of <code>page</code>; it is nested under <code>revision</code>.</p> SourceResult Python<pre><code># Print the tree of page, to find the path \nprint_tags_tree(page, only_tagnames=True)\n</code></pre> Python Console Session<pre><code> 0 page\n      1 title\n      1 ns\n      1 id\n      1 revision\n           2 id\n           2 parentid\n           2 timestamp\n           2 contributor\n                3 username\n                3 id\n           2 comment\n           2 origin\n           2 model\n           2 format\n           2 text\n           2 sha1\n</code></pre> <p>Fortunately, the <code>find</code> method allows us to specify the path to a nested tag. In this case, we specify the path <code>revision/text</code> from <code>page</code> to <code>text</code>:</p> SourceResult Python<pre><code>wikitext = page.find('revision/text', NAMESPACES)\nprint(wikitext)\n# Let's print the first 300 characters of the wikitext\nprint(wikitext.text[:300])\n</code></pre> Python Console Session<pre><code>&lt;Element {http://www.mediawiki.org/xml/export-0.11/}text at 0x7f89e6a3be80&gt;\n{{Siehe auch|[[schon]]}}\n{{Wort der Woche|26|2007}}\n== sch\u00f6n ({{Sprache|Deutsch}}) ==\n=== {{Wortart|Adjektiv|Deutsch}} ===\n\n{{Deutsch Adjektiv \u00dcbersicht\n|Positiv=sch\u00f6n\n|Komparativ=sch\u00f6ner\n|Superlativ=sch\u00f6nsten\n|Bild 1=Jaguar E-type (serie III).jpg|mini|1|ein ''sch\u00f6nes'' [[Auto]]\n|Bild 2=12er Anitra \n</code></pre> <p>We are done; we have just retrieved the wikitext content from the XML string.</p> <p>Before moving on to the next section, let us quickly recap what we have learned by using functions.</p> SourceResult Python<pre><code>import requests\nimport lxml.etree as ET\n\ndef fetch(title):\n    url = f'https://de.wiktionary.org/wiki/Spezial:Exportieren/{title}'\n    headers = {\n        \"User-Agent\": \"Search for German words (https://lennon-c.github.io/python-wikitext-parser-guide)\"\n    }\n    resp = requests.get(url, headers=headers)\n    resp.raise_for_status()\n    return resp.text\n\ndef fetch_wikitext(title):\n    xml_content = fetch(title)\n    root = ET.fromstring(xml_content)\n    namespaces  = root.nsmap\n    page = root.find('page', namespaces)\n    wikitext = page.find('revision/text', namespaces)\n    return wikitext.text \n\n# let us try it \nprint(fetch_wikitext('sch\u00f6n')[:5000])\n</code></pre> Python Console Session<pre><code>{{Siehe auch|[[schon]]}}\n{{Wort der Woche|26|2007}}\n== sch\u00f6n ({{Sprache|Deutsch}}) ==\n=== {{Wortart|Adjektiv|Deutsch}} ===\n\n{{Deutsch Adjektiv \u00dcbersicht\n|Positiv=sch\u00f6n\n|Komparativ=sch\u00f6ner\n|Superlativ=sch\u00f6nsten\n|Bild 1=Jaguar E-type (serie III).jpg|mini|1|ein ''sch\u00f6nes'' [[Auto]]\n|Bild 2=12er Anitra US5 Kiel2009.jpg|mini|1|eine ''sch\u00f6ne'' [[Yacht]]\n|Bild 3=Gedeon Burkhard Berlinale 2008.jpg|mini|1|ein ''sch\u00f6ner'' [[Mann]]\n}}\n\n{{Worttrennung}}\n:sch\u00f6n, {{Komp.}} sch\u00f6\u00b7ner, {{Sup.}} am sch\u00f6ns\u00b7ten\n\n{{Aussprache}}\n:{{IPA}} {{Lautschrift|\u0283\u00f8\u02d0n}}\n:{{H\u00f6rbeispiele}} {{Audio|De-sch\u00f6n.ogg}}, {{Audio|De-sch\u00f6n fcm.ogg}}, {{Audio|De-sch\u00f6n2.ogg}}, {{Audio|De-at-sch\u00f6n.ogg|spr=at}}\n:{{Reime}} {{Reim|\u00f8\u02d0n|Deutsch}}\n\n{{Bedeutungen}}\n:[1] \u00e4sthetisch, eine angenehme Wirkung auf die Sinne habend: zum Beispiel ein gutes Aussehen habend, sich gut anh\u00f6rend\n:[2] {{K|allgemein}} angenehm, gut, anst\u00e4ndig\n:[3] {{K|umgangssprachlich}} verst\u00e4rkend im Sinne von [[betr\u00e4chtlich]]\n:[4] {{K|umgangssprachlich}} zustimmende Antwort auf eine Frage\n:[5] {{K|umgangssprachlich}} so, wie es sich geh\u00f6rt\n:[6] in festen Wendungen mit verschwommener Bedeutung [1, 2]\n\n{{Herkunft}}\n:{{goh.}} ''sc\u014dni'' \u201eansehnlich, gl\u00e4nzend, rein, herrlich\u201c, {{gmh.}} ''sch\u0153n, sch\u0153ne'' auch \u201eschonend, freundlich\u201c, {{mlg.}} ''sch\u00f6n, sch\u00f6ne'', {{dum.}} ''sc\u014dne''&lt;ref&gt;{{Lit-Pfeifer: Etymologisches W\u00f6rterbuch|A=6}}, Seite 1236.&lt;/ref&gt;\n\n{{Synonyme}}\n:[1] [[ansprechend]], [[anziehend]], [[\u00e4sthetisch]], [[attraktiv]], [[h\u00fcbsch]], [[dekorativ]]\n:[2] [[angenehm]], [[gut]]\n:[3] [[besonders]], [[betr\u00e4chtlich]], [[sehr]], [[\u00fcberaus]]\n:[4] [[einverstanden]], [[okay]]\n:[5] [[ordnungsgem\u00e4\u00df]]\n\n{{Gegenw\u00f6rter}}\n:[1] [[h\u00e4sslich]], [[unsch\u00f6n]]\n:[2] [[schlecht]], [[unangenehm]]\n:[3] [[nicht]]\n:[4] [[nein]]\n:[5] {{\u00f6sterr.|:}} [[schirch]]\n\n{{Unterbegriffe}}\n:[1] [[formsch\u00f6n]]\n:[1, 2] [[wundersch\u00f6n]]\n\n{{Beispiele}}\n:[1] Sie hat ''sch\u00f6nes'' Haar. Das Musikst\u00fcck ist ''sch\u00f6n.''\n:[1] Sie sang ''sch\u00f6n,'' ''sch\u00f6ner'' als gew\u00f6hnlich, weil die Instrumentalisten ihr so vertraut waren. Am ''sch\u00f6nsten'' sang sie, als Viktor am Klavier sa\u00df.\n:[1] \u201eNik Wallenda, Urenkel eines deutschen Zirkusakrobaten, hat als erster Mensch die Niagaraf\u00e4lle an ihrer ''sch\u00f6nsten'' und gef\u00e4hrlichsten Stelle \u00fcberquert.\u201c&lt;ref&gt;{{Per-Zeit Online|Online=https://www.zeit.de/zustimmung?url=https%3A%2F%2Fwww.zeit.de%2Fgesellschaft%2Fzeitgeschehen%2F2012-06%2Fakrobat-niagarafaelle-balance |Autor=Zeit Online |Titel= Zu Fu\u00df \u00fcbers gro\u00dfe Wasser |Tag=16 |Monat= 06|Jahr= 2012|zugriff=2020-04-03}}&lt;/ref&gt;\n:[1, 2] Sille ist ''sch\u00f6n.''\n:[1, 2] \u201eDie Herzen bebten \u00fcber die K\u00fchnheit des jungen, ''sch\u00f6nen'', wagemutigen Paares.\u201c&lt;ref&gt;{{Literatur | Autor= Karl May| Titel= Winnetou IV | Verlag= Neues Leben | Ort= Berlin | Jahr= 1993 [1910] }}, Seite 429.&lt;/ref&gt;\n:[2] Das hat er aber ''sch\u00f6n'' gemacht. Wir hatten ''sch\u00f6ne'' Ferientage. Es w\u00e4re ''sch\u00f6n,'' wenn wir uns wieder treffen. Es war ''sch\u00f6n'' von ihm, seiner Frau Blumen zu schenken.\n:[2] Das ist ja eine ''sch\u00f6ne'' Geschichte! Oder anders gesagt: Das ist aber wirklich schlimm!\n:[2] Du bist mir ja ein ''sch\u00f6ner'' Freund! Oder anders gesagt: Du bist wahrlich ein schlechter Freund!\n:[3] Da wird sie ganz ''sch\u00f6n'' staunen. Also, da wird sie aber \u00fcberrascht sein.\n:[3] Das wird eine ''sch\u00f6ne'' Stange Geld kosten. Also, das wird wohl ziemlich teuer werden.\n:[4] Lass uns doch mal wieder im Kino einen Film ansehen! \u2013 ''Sch\u00f6n,'' dann komm!\n:[5] So, jetzt gehen wir ''sch\u00f6n'' ins Bett.\n:[5] ''Sch\u00f6n'' aufpassen, wenn du \u00fcber die Stra\u00dfe gehst!\n\n{{Redewendungen}}\n:[[das sch\u00f6ne Geschlecht|das ''sch\u00f6ne'' Geschlecht]] \u2013 die Frauen in ihrer Gesamtheit\n:[[die sch\u00f6nen K\u00fcnste|die ''sch\u00f6nen'' K\u00fcnste]] \u2013 Dichtung, Musik, Malerei, Bildhauerei\n:[[eine sch\u00f6ne Leich]] \u2013 \u2026\n:[[jemandem sch\u00f6ne Augen machen|jemandem ''sch\u00f6ne'' Augen machen]]\n:[[sch\u00f6ne Worte machen|''sch\u00f6ne'' Worte machen]] \u2013 schmeicheln\n:[[das ist zu sch\u00f6n, um wahr zu sein|Das ist zu ''sch\u00f6n'', um wahr zu sein]] \u2013 \u2026\n:[[wie es so sch\u00f6n hei\u00dft|wie es so ''sch\u00f6n'' hei\u00dft]] \u2013 \u2026\n:[[immer sch\u00f6n der Reihe nach|immer ''sch\u00f6n'' der Reihe nach]] \u2013 \u2026\n:[[sch\u00f6n ist, was gef\u00e4llt|''Sch\u00f6n'' ist, was gef\u00e4llt]]. \u2013 \u2026\n:[[sch\u00f6n und gut|''sch\u00f6n'' und gut]] \u2013 Zustimmung zu einem Argument, gefolgt von \u201eAber \u2026\u201c\n\n{{Sprichw\u00f6rter}}\n:[1] [[aus einem sch\u00f6nen Morgen wird selten ein sch\u00f6ner Tag, aus einem sch\u00f6nen M\u00e4dchen wird meistens ein Schlumpersack|Aus einem ''sch\u00f6nen'' Morgen wird selten ein ''sch\u00f6ner'' Tag, aus einem ''sch\u00f6nen'' M\u00e4dchen wird meistens ein Schlumpersack]].\n\n{{Charakteristische Wortkombinationen}}\n:[1] ''sch\u00f6ne'' [[Auge]]n, ''sch\u00f6ne'' [[Bescherung]], ''sch\u00f6ne'' [[Tag]]e, ''sch\u00f6nes'' [[Wetter]], [[traumhaft]] ''sch\u00f6n'', [[atemberaubend]] ''sch\u00f6n''\n:[3] ganz ''sch\u00f6n'' [[frech]], [[bitte]] ''sch\u00f6n'', [[danke]] ''sch\u00f6n'', [[recht]] ''sch\u00f6nen'' [[Dank]], ''sch\u00f6ne'' [[Gru\u00df|Gr\u00fc\u00dfe]] [[ausrichten]]\n\n{{Wortbildungen}}\n:[[besch\u00f6nigen]], [[bildsch\u00f6n]], [[Bittesch\u00f6n]], [[Dankesch\u00f6n]], [[formsch\u00f6n]], [[sch\u00f6nen]], [[sch\u00f6nf\u00e4rben]], [[Sch\u00f6ngeist]], [[Sch\u00f6nheit]], [[Sch\u00f6nling]], [[sch\u00f6nmachen]], [[sch\u00f6nred\n</code></pre>"}]}